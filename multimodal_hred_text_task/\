import math
import sys
import os
sys.path.append(os.getcwd())
import read_data
import math
import pickle as pkl
import random
import os.path
import params
from params import *
import read_data
from read_data_task1 import *
from hierarchy_model import *
import nltk

def feeding_dict(model, inputs_text, inputs_image, target_text, decoder_input, weights, feed_prev):
        feed_dict = {}
        for i,j in zip(model.encoder_text_inputs, inputs_text):
            feed_dict[i] = j
        for i,j in zip(model.encoder_img_inputs, inputs_image):
            feed_dict[i] = j
        for i,j in zip(model.target_text, target_text):
            feed_dict[i] = j
        for i,j in zip(model.decoder_text_inputs, decoder_input):
            feed_dict[i] = j
        for i,j in zip(mode.text_weights, weights):
            feed_dict[i] = j
        feed_dict[model.feed_previous] = feed_prev    
        return feed_dict
    
def check_dir(param):
    '''Checks whether model and logs directtory exists, if not then creates both directories for saving best model and logs.
    Args:
        param:parameter dictionary.'''
    if not os.path.exists(param['logs_path']):
        os.makedirs(param['logs_path'])    
    if not os.path.exists(param['model_path']):
        os.makedirs(param['model_path'])

def get_test_op(sess, model, batch_dict, param, logits, losses):
    avg_batch_loss = 0
    test_batch_text, test_batch_image, batch_text_target, batch_decoder_input, batch_text_weight = read_data_task1.get_batch_data(param['max_len'], param['max_images'], param['max_utter'], param['batch_size'], batch_dict)
    feed_dict = feeding_dict(model, test_batch_text, test_batch_image, batch_text_targets, batch_decoder_input, batch_text_weight, True)
    dec_op, loss = sess.run([logits, losses], feed_dict=feed_dict)
    for sentence_loss in loss:
        avg_batch_loss = avg_batch_loss + sentence_loss
    return dec_op, avg_batch_loss
    
def write_to_file(pred_op, true_op):
    pred_file = ''
    true_file = ''
    with open(true_file, 'w') as f_true:
        for true_sentence in true_op:
            f_true.write(true_sentence.strip()+'\n')

    with open(pred_file, 'w') as f_pred:
        for pred_sentence in pred_op:
            f_pred.write(pred_sentence.strip()+'\n')
    print 'Test (true and predicted) output written to corresponding files'
    
def perform_test(sess, model, saver, model_file, get_pred_sentence, param, logits, losses):
    print 'reading model from  modelfile'
    saver.restore(sess, model_file)
    test_data_file = pkl.load(open(param['test_data_file']),'rb')
    #test_data_file = read_data_task1.check_padding(test_data_file, param)
    print "Test dialogues loaded"
    predicted_sentence = []
    test_loss = 0
    n_batches = len(test_dialogue_dict)/param['batch_size']
    test_text_targets = read_data_task1.load_valid_test_target(param['test_data_file'])
    for i in range(n_batches):
        batch_dict = test_dialogue_dict[i*param['batch_size'], (i+1)*param['batch_size']]
        test_op, avg_batch_loss = get_test_op(sess, model, batch_dict, param, logits, losses)
        test_loss = avg_batch_loss + test_loss
        predicted_sentence.append(get_predicted_sentence(test_op))
    test_predicted_sentence = predicted_sentence[0:len(test_text_targets)]
    write_to_file(test_predicted_sentence, test_text_targets)
    print ('average test loss is =%.6f' %(float(test_loss)/float(n_batches)))
    sys.stdout.flush() 

def run_training(param):

    def get_train_loss(model, batch_dict):
        train_batch_text, train_batch_image, batch_text_target, batch_decoder_input, batch_text_weight = read_data_task1.get_batch_data(param['max_len'], param['max_images'], param['max_utter'], param['batch_size'], batch_dict)
        if epoch<=2:
            feed_dict = feeding_dict(model, train_batch_text, train_batch_image, batch_text_target, batch_decoder_input, batch_text_weight, False)
        else:
            feed_dict = feeding_dict(model, train_batch_text, train_batch_image, batch_text_target, batch_decoder_input, batch_text_weight, True)
        loss, dec_op, _ = sess.run([losses, logits, train_op], feed_dict=feed_dict)
        return loss, dec_op

    def get_valid_loss(model, batch_dict):
        valid_batch_text, valid_batch_image, batch_text_target, batch_decoder_input, batch_text_weight = read_data_task1.get_batch_data(param['max_len'], param['max_images'], param['max_utter'], param['batch_size'], batch_dict)
        feed_dict = feeding_dict(model, valid_batch_text, valid_batch_image, batch_text_target, batch_decoder_input, batch_text_weight, True)
        loss, dec_op, _ = sess.run([losses, logits], feed_dict)
        return loss, dec_op

    def get_avg_batch_loss(batch_loss):
        return np.average(np.asarray(batch_loss))     

    def perform_training(model, batch_dict):
        batch_train_loss, dec_op = get_train_loss(model, batch_dict)
        avg_batch_loss = get_avg_batch_loss(batch_train_loss)
        return avg_batch_loss

    def perform_evaluation(model, batch_dict, batch_text_targets, epoch, step):
        batch_valid_loss, valid_op = get_valid_loss(model, batch_dict)
        avg_batch_loss = get_avg_batch_loss(batch_valid_loss)    
        batch_predicted_sentence = get_predicted_sentence(valid_op)
        print_predicted_true_op(batch_predicted_sentence, batch_text_targets, step, epoch, batch_valid_loss)
        return avg_batch_loss

    def evaluate(epoch, step, valid_text_targets):
        print 'Validation started'
        sys.stdout.flush()
        valid_loss = 0
        batch_predicted_sentence=[] 
        n_batches = float(len(valid_dialogue_dict))/float(param['batch_size']) 
        for i in range(n_batches):
            start_index = i*param['batch_size']
            end_index = min(len(valid_dialogue_dict, (i+1)*param['batch_size']))
            pad_size = 0
            if (end_index-start_index)<param['batch_size']:
                pad_size = param['batch_size'] - (end_index-start_index)
            batch_dict = valid_dialogue_dict[start_index:end_index]
            batch_target_sentences = valid_text_targets[start_index:end_index]
            if pad_size>0:
                batch_dict = batch_padding(batch_dict, pad_size)
                batch_target_sentences = batch_padding(batch_target_sentences, pad_size)
            avg_batch_loss = perform_evaluation(batch_dict, batch_target_sentences, epoch, step)
            valid_loss = valid_loss + avg_batch_loss
        return float(valid_loss)/float(n_batches)
        
    def print_pred_true_op(pred_op, true_op, step, epoch, batch_valid_loss):
        for i in range(len(true_op)):
            print "true sentence in step "+str(step)+" of epoch "+str(epoch)+" is:"
            sys.stdout.flush()
            print true_op[i]
            print "\n"
            print "predicted sentence in step "+str(step)+" of epoch "+str(epoch)+" is:"
            sys.stdout.flush()
            print pred_op[i]
            print "\n"
            sys.stdout.flush()
            print "loss for the pair of true and predicted sentences", str(batch_valid_loss[i])
            print "\n"

    def map_id_to_word(word_indices):
        sentence_list = []
        for sent in word_indices:
            word_list = []
            for word_index in sent:
                word = train_dict[word_index]
                word_list.append(word)
            sentence_list.append(" ".join(word_list))
        return sentence_list

    def get_predicted_sentence(valid_op):
        max_prob_index = []
        for op in valid_op:
            max_index = np.argmax(op, axis=1)
            max_prob_index.append(max_index)
        max_prob_arr = np.transpose(max_prob_index)
        pred_sentence_list = map_id_to_word(max_prob_arr)
        return pred_sentence_list

    train_data_file = pkl.load(open(param['train_data_file']))
    print 'Train dialogue dataset loaded'
    #train_data_file = read_data_task1.check_padding(train_data_file, param)
    sys.stdout.flush()
    train_dict = pkl.load(open(param['vocab_file'], "rb"))
    print "Vocab dictionary loaded"
    sys.stdout.flush()
    valid_data_file = pkl.load(open(param['valid_data_file']))
    print 'Valid dialogue dataset loaded'
    sys.stdout.flush()
    #valid_data_file = read_data_task1.check_padding(valid_data_file, param)
    valid_text_targets = read_data_task1.load_valid_test_target(param['valid_data_file'])
    print 'valid target sentence list loaded'
    print 'writing terminal output to file'
    f_out = open(param['terminal_op'],'w')
    sys.stdout=f_out
    check_dir(param)
    n_batches = len(train_dialogue_dict)/param['batch_size']
    model_file = os.path.join(param['model_path'],"best_model")
    
    with tf.Graph().as_default():
        model = Hierarchical_seq_model('text', param['text_embedding_size'], param['image_embedding_size'], param['image_rep_size'], param['cell_size'], param['batch_size'], param['learning_rate'], param['max_len'], param['max_utter'], param['max_images'], param['patience'], param['decoder_words'], param['max_gradient_norm'], param['activation'])   
        model.create_placeholder()
        logits = model.inference()
        losses = model.loss(logits)
        train_op = model.train(losses)
        print "model created"
        sys.stdout.flush()
        saver = tf.train.Saver()
        init = tf.initialize_all_variables()
        sess = tf.Session()
        if os.path.isfile(sess, model_file):
            print "best model exists.. restoring from that point"
            saver.restore(sess, model_file)
        else:
            print "initializing fresh variables"
            sess.run(init)
        best_valid_loss = float("inf")
        best_valid_epoch=0
        all_var = tf.all_variables()
        print 'printing all' , len(all_var),' TF variables:'
        for var in all_var:
            print var.name, var.get_shape()
        print 'training started'
        sys.stdout.flush()

        for epoch in range(param['max_epochs']):
            random.shuffle(train_dialogue_dict)
            train_loss=0
            for i in range(n_batches):
                train_batch_dict=train_dialogue_dict[i*param['batch_size'], (i+1)*param['batch_size']]
                avg_batch_loss = perform_training(model, train_batch_dict)
                print('Epoch  %d Step %d train loss (avg over batch) =%.6f' %(epoch, i, avg_batch_loss))
                sys.stdout.flush()
                train_loss = train_loss + avg_batch_loss
                avg_train_loss = float(train_loss)/float(i)            
                print ('Epoch %d Step %d Average training loss till now (avg over batch)=%.6f' %(epoch, i, avg_train_loss))
                if i>0 and i%param['valid_freq']==0:
                    valid_loss = evaluate(epoch, i, valid_text_targets)
                    print ('Epoch %d Step %d valid loss= %.6f' %(epoch, i, valid_loss))
                    sys.stdout.flush()
                    if best_valid_loss>valid_loss:
                        saver.save(sess, model_file)
                        best_valid_loss=valid_loss
                    else:
                        continue
            print 'epoch '+str(epoch)+' of training is completed'
            sys.stdout.flush()
        print 'Training over'
        print 'Evaluating on test data'
    f_out.close()
    
    

def main():
    param = get_params()
    if os.path.exists(param['train_data_file']) and os.path.exists(param['valid_data_file']) and os.path.exists(param['test_data_file']):
        print 'dictionary already exists'
        sys.stdout.flush()
    else:
        get_dialog_dict(param)
        print 'dictionary formed'
        sys.stdout.flush()
    run_training(param)
    
if __name__=="__main__":
    main()            



                    


    

































